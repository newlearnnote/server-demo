# 메시지 처리 흐름

이 문서는 Nura 시스템에서 메시지 생성 및 AI 응답 생성의 전체 흐름을 설명합니다.

## 개요

사용자가 메시지를 보내면 시스템은 다단계 프로세스를 거칩니다:
1. 사용자 메시지 검증 및 저장
2. 컨텍스트로 사용할 문서 결정
3. **대화 맥락 유지를 위한 대화 히스토리 조회**
4. 관련 문서 청크 검색 (문서가 존재하는 경우)
5. 사용 가능한 컨텍스트 및 대화 히스토리를 기반으로 AI 응답 생성
6. AI 응답 저장 및 반환

시스템은 세 가지 시나리오를 지능적으로 처리합니다:
- **문서 기반 Q&A**: 관련 문서 청크가 발견된 경우
- **일반 대화 폴백**: 문서는 존재하지만 관련 청크를 찾지 못한 경우
- **일반 대화**: 사용 가능한 문서가 없는 경우

**핵심 기능**: 각 채팅 세션은 독립적인 대화 히스토리를 유지하여 AI가 컨텍스트, 후속 질문, 이전 메시지에 대한 참조(예: "그거", "위에서 말한")를 이해할 수 있습니다.

---

## 전체 흐름 다이어그램

```
[클라이언트]
   │
   │ POST /api/v1/messages
   │ {
   │   chat_id?: string,
   │   content: string,
   │   document_ids?: string[],
   │   category_id?: string
   │ }
   │
   ▼
[MessageRouter]
   │
   ▼
[MessageService.create_message]
   │
   ├─► [단계 1: 채팅 관리]
   │    │
   │    ├─► [chat_id가 null인 경우]
   │    │    ├─ 새 채팅 생성
   │    │    ├─ 제목 생성 (내용의 첫 50자)
   │    │    ├─ 문서 연결 (document_ids가 제공된 경우)
   │    │    └─ 카테고리 검증 (category_id가 제공된 경우)
   │    │
   │    └─► [chat_id가 존재하는 경우]
   │         └─ 채팅 소유권 검증
   │
   ├─► [단계 2: 사용자 메시지 저장]
   │    └─ role="user"로 메시지 생성
   │
   ├─► [단계 3: RAG용 문서 수집]
   │    │
   │    ├─► [우선순위 1: 명시적 document_ids]
   │    │    ├─ 요청에 document_ids가 제공된 경우
   │    │    ├─ 메시지에 첨부 (MessageDocument)
   │    │    └─ COMPLETED 문서만 필터링
   │    │
   │    ├─► [우선순위 2: 채팅 문서]
   │    │    ├─ document_ids가 제공되지 않은 경우
   │    │    ├─ 해당 채팅의 ChatDocument 조회
   │    │    └─ COMPLETED 문서만 필터링
   │    │
   │    └─► [우선순위 3: 사용자의 모든 문서]
   │         ├─ 채팅에 문서가 없는 경우
   │         ├─ 사용자의 모든 문서 조회
   │         └─ COMPLETED 문서만 필터링
   │
   └─► [단계 4: AI 응답 생성]
        │
        ▼
     [_generate_ai_response]
        │
        ├─► [대화 히스토리 조회] ✅ 신규
        │    ├─ 해당 채팅의 최근 메시지 조회
        │    ├─ MAX_CONVERSATION_HISTORY 쌍만큼 제한 (기본값: 10)
        │    ├─ 생성 시간 순서대로 정렬 (오래된 것부터)
        │    └─ role과 content만 추출
        │
        ├─► [케이스 A: 문서 사용 가능]
        │    │
        │    ├─► [RAGService.search_similar_chunks]
        │    │    ├─ 문서 ID 추출
        │    │    ├─ 쿼리 임베딩 생성 (text-embedding-3-small)
        │    │    ├─ 유사도를 기반으로 ChromaDB 검색
        │    │    ├─ document_ids로 필터링
        │    │    └─ 상위 4개 청크 반환
        │    │
        │    ├─► [분기: 청크 발견됨]
        │    │    │
        │    │    ├─► [RAGService.generate_response]
        │    │    │    ├─ 대화 히스토리 포맷팅 ✅ 신규
        │    │    │    ├─ 청크에서 컨텍스트 구축
        │    │    │    ├─ 유연한 프롬프트 생성:
        │    │    │    │   • 대화 히스토리
        │    │    │    │   • 문서 컨텍스트
        │    │    │    │   • 현재 질문
        │    │    │    └─ GPT-4o-mini 호출 (temp=0.0, max_tokens=1000)
        │    │    │
        │    │    └─► [출처 인용 구축]
        │    │         ├─ 메타데이터 추출 (document_id, filename, chunk_index)
        │    │         ├─ chunk_id = "{document_id}_{chunk_index}" 생성
        │    │         └─ content_preview 추가 (첫 200자)
        │    │
        │    └─► [분기: 청크 발견되지 않음]
        │         │
        │         └─► **케이스 B로 폴백** (일반 대화)
        │
        └─► [케이스 B: 문서 없음 또는 관련 청크 없음]
             │
             └─► [RAGService.generate_response]
                  ├─ 대화 히스토리 포맷팅 ✅ 신규
                  ├─ 일반 대화 프롬프트 생성:
                  │   • 대화 히스토리
                  │   • 현재 질문
                  ├─ GPT-4o-mini 호출 (temp=0.0, max_tokens=1000)
                  └─ sources = null
```

---

## 세부 단계 설명

### 단계 1: 채팅 관리

**목적**: 메시지가 유효한 채팅 세션에 속하는지 확인합니다.

**새 채팅 생성:**
- `chat_id`가 null일 때 트리거됨
- 제목은 사용자 메시지의 첫 50자에서 자동으로 생성됨
- `document_ids`가 제공되면 ChatDocument 테이블을 통해 문서를 채팅에 연결
- `category_id`가 제공되면 카테고리를 검증하고 연결

**기존 채팅:**
- 사용자가 채팅을 소유하고 있는지 검증
- 무단 접근 방지

---

### 단계 2: 사용자 메시지 저장

**목적**: 대화 히스토리를 위해 사용자의 입력을 기록합니다.

**프로세스:**
```python
user_message = Message(
    chatId=chat_id,
    role=MessageRole.USER,
    content=message_data.content,
    sources=None
)
```

**참고**: 사용자 메시지는 출처가 없으며, 어시스턴트 메시지만 출처를 가집니다.

---

### 단계 3: 대화 히스토리 조회 ✅ 신규

**목적**: 최근 메시지 히스토리를 조회하여 맥락 있는 대화를 가능하게 합니다.

**프로세스:**
```python
conversation_history = await MessageService._get_conversation_history(
    db,
    chat_id,
    limit=settings.MAX_CONVERSATION_HISTORY  # 기본값: 10쌍
)
```

**구성:**
- `MAX_CONVERSATION_HISTORY`: 포함할 최대 사용자-어시스턴트 메시지 쌍 수 (기본값: 10)
- 쿼리는 최근 `limit * 2`개 메시지 조회 (사용자 + 어시스턴트 쌍)
- 적절한 컨텍스트 흐름을 위해 시간 순서대로 정렬 (오래된 것부터)

**이점:**
- AI가 "그거", "위에서 말한", "이전 예시" 같은 참조 이해
- 컨텍스트를 다시 설명하지 않고 자연스러운 후속 질문 가능
- 각 채팅 세션이 독립적인 대화 히스토리 유지

---

### 단계 4: 문서 수집 우선순위

**목적**: RAG 컨텍스트로 사용할 문서를 결정합니다.

**우선순위 시스템:**

1. **명시적 `document_ids`** (최우선)
   - 사용자가 이 메시지를 위해 특정 문서를 선택
   - 가장 정확하고 의도적인 선택
   - MessageDocument 테이블을 통해 메시지에 문서 첨부

2. **채팅 연결 문서** (중간 우선순위)
   - 채팅 생성 시 연결된 문서
   - 전체 대화에 대한 지속적인 컨텍스트
   - ChatDocument 테이블을 통해 조회

3. **사용자의 모든 문서** (폴백)
   - 채팅에 특정 문서가 없을 때 사용
   - "내 문서 중 아무거나 질문" 동작 활성화
   - 사용 가능한 모든 지식 자동 활용

**필터링:**
- `status=COMPLETED`인 문서만 사용
- `status=PROCESSING` 또는 `FAILED`인 문서는 제외

---

### 단계 5: 문서와 함께 RAG

**사용 시기**: 쿼리에 사용할 수 있는 문서가 있는 경우.

**청크 검색 프로세스:**
```python
# 1. 문서 ID 추출
document_ids = [doc.id for doc in documents_for_rag]

# 2. 유사 청크 검색
context_chunks = await RAGService.search_similar_chunks(
    query=user_query,
    document_ids=document_ids  # 이 문서들로 필터링
)
```

**벡터 검색 구성:**
- 임베딩 모델: OpenAI text-embedding-3-small
- Top K: 4개 청크
- 저장소: ChromaDB (로컬 영구 저장)
- 유사도 메트릭: 코사인 유사도

**청크가 발견된 경우:**

다음을 포함하는 유연한 프롬프트를 생성합니다:
1. **대화 히스토리** - 컨텍스트 연속성을 위해 ✅ 신규
2. 참조를 위한 문서 내용
3. 현재 질문

```python
# 대화 히스토리 포맷팅
history_text = "\n\n이전 대화 내용:\n"
for msg in conversation_history:
    role_name = "사용자" if msg["role"] == "user" else "어시스턴트"
    history_text += f"{role_name}: {msg['content']}\n"

# 프롬프트 구축
prompt = f"""당신은 친절한 AI 어시스턴트입니다.
{history_text}

참고 문서 내용:
{context}

사용자 질문: {query}

답변 시 주의사항:
1. 위 대화 히스토리를 참고하여 맥락을 이해하세요.
2. 문서 내용이 질문과 관련이 있다면 우선적으로 참고하여 답변하세요.
3. 문서 내용을 사용했다면 어떤 문서를 참고했는지 간단히 언급하세요.
4. 문서 내용이 질문과 관련이 없다면, 일반 지식을 바탕으로 친절하게 답변하세요.
5. 한국어로 답변하세요.
"""
```

**출처 인용:**
응답에 사용된 각 청크는 다음을 포함합니다:
```typescript
{
  document_id: string;       // 문서 식별자
  document_name: string;     // 원본 파일명
  chunk_id: string;          // "{document_id}_{chunk_index}"
  page: number | null;       // 페이지 번호 (사용 가능한 경우)
  similarity: number;        // 유사도 점수
  content_preview: string;   // 청크의 첫 200자
}
```

---

### 단계 6: 일반 대화로 폴백

**트리거 조건:**
1. 사용 가능한 문서가 없거나
2. 문서는 존재하지만 관련 청크를 찾지 못한 경우

**이전 동작 (v1):**
```python
# ❌ 이전: 답변 거부
assistant_content = "관련된 정보를 찾을 수 없습니다."
```

**현재 동작 (v3 - 대화 히스토리 포함):** ✅ 업데이트됨
```python
# ✅ 새로운: 대화 컨텍스트와 함께 일반 지식으로 폴백
assistant_content = await RAGService.generate_response(
    query=user_query,
    context_chunks=[],  # 빈 컨텍스트 = 일반 모드
    conversation_history=conversation_history  # 히스토리 포함
)
```

**일반 대화 프롬프트:**
```python
# 대화 히스토리 포맷팅
history_text = "\n\n이전 대화 내용:\n"
for msg in conversation_history:
    role_name = "사용자" if msg["role"] == "user" else "어시스턴트"
    history_text += f"{role_name}: {msg['content']}\n"

prompt = f"""당신은 친절한 AI 어시스턴트입니다.
{history_text}

사용자 질문: {query}

답변 시 주의사항:
1. 위 대화 히스토리를 참고하여 맥락을 이해하세요.
2. 친절하고 정확하게 답변하세요.
3. 한국어로 답변하세요.
4. 모르는 내용은 모른다고 솔직히 말하세요.
"""
```

**결과:**
- 출처 인용 없음 (sources = null)
- 일반 지식을 기반으로 한 순수 LLM 응답
- 거부 대신 유용한 답변 제공

---

## 예시 시나리오

### 시나리오 1: 문서 기반 Q&A

**사용자 메시지:**
```
"Attention 메커니즘을 요약해줘"
```

**시스템 동작:**
1. Attention.md 문서 찾기
2. Attention 메커니즘에 대한 관련 청크 검색
3. 4개의 관련 청크 발견
4. 문서 내용을 사용하여 응답 생성
5. 출처 인용 포함

**응답:**
```
Attention 메커니즘은 입력 문장의 각 단어에 대해 동적으로 가중치를 계산하여...

[참고: Attention.md]
```

**출처:** `[{ document_id: "...", document_name: "Attention.md", ... }]`

---

### 시나리오 2: 일반 지식으로 폴백

**사용자 메시지:**
```
"후쿠오카 여행 일정 짜줘"
```

**시스템 동작:**
1. Attention.md 문서 찾기 (사용자가 업로드한 유일한 문서)
2. "후쿠오카 여행"과 관련된 청크 검색
3. 관련 청크를 찾지 못함 (문서는 AI에 관한 것이지 여행이 아님)
4. **일반 대화 모드로 폴백**
5. LLM의 일반 지식을 사용하여 응답 생성

**응답:**
```
후쿠오카 2박 3일 여행 일정을 추천드립니다:

1일차: 하카타역 주변 탐방, 이치란 라멘...
```

**출처:** `null` (문서 출처 사용하지 않음)

---

### 시나리오 3: 순수 일반 대화

**사용자 메시지:**
```
"안녕하세요"
```

**시스템 동작:**
1. 사용 가능한 문서 없음
2. RAG를 완전히 건너뜀
3. 일반 대화 모드 사용

**응답:**
```
안녕하세요! 무엇을 도와드릴까요?
```

**출처:** `null`

---

## 주요 개선 사항

### v1 → v2: 지능형 폴백

**v1의 문제점:**
사용자가 주제 A에 대한 문서를 업로드했을 때, AI가 주제 B에 대한 질문에 답변을 거부했습니다:

**예시:**
- 업로드: `Attention.md` (AI/ML 주제)
- 질문: "후쿠오카 여행 일정 짜줘"
- 응답: ❌ "관련된 정보를 찾을 수 없습니다"

### v2의 해결책:
시스템이 이제 지능적으로 일반 지식으로 폴백합니다:

**동일한 예시:**
- 업로드: `Attention.md` (AI/ML 주제)
- 질문: "후쿠오카 여행 일정 짜줘"
- 응답: ✅ "후쿠오카 2박 3일 여행 일정을 추천드립니다..."

**이점:**
- AI가 관련 없는 문서로 인해 "차단"되지 않음
- 사용자가 언제든지 무엇이든 질문 가능
- 관련이 있을 때는 문서 기반 Q&A
- 관련이 없을 때는 일반 대화
- 매끄러운 하이브리드 경험

---

### v2 → v3: 대화 히스토리 컨텍스트 ✅ 신규

**v2의 문제점:**
AI가 이전 대화 컨텍스트를 고려하지 않고 각 메시지에 독립적으로 응답했습니다:

**예시:**
- 사용자: "Python 리스트 컴프리헨션 설명해줘"
- AI: [리스트 컴프리헨션 설명]
- 사용자: "그거의 장점은 뭐야?"
- AI: ❌ "무엇을 말씀하시는지 잘 모르겠습니다"

**v3의 해결책:**
시스템이 이제 각 채팅 세션의 대화 히스토리를 유지합니다:

**동일한 예시:**
- 사용자: "Python 리스트 컴프리헨션 설명해줘"
- AI: [리스트 컴프리헨션 설명]
- 사용자: "그거의 장점은 뭐야?"
- AI: ✅ "리스트 컴프리헨션의 장점은..." ("그거"가 리스트 컴프리헨션을 가리킨다는 것을 이해)

**구현 방식:**
- 각 채팅 세션은 최근 대화 히스토리를 조회 (기본값: 10 메시지 쌍)
- 컨텍스트 인식 응답을 위해 프롬프트에 히스토리 포함
- 채팅 세션별 독립적인 컨텍스트 (채팅방 A가 채팅방 B에 영향 없음)
- `MAX_CONVERSATION_HISTORY` 설정을 통한 토큰 제한 제어

**이점:**
- 재설명 없이 자연스러운 후속 질문 가능
- AI가 "그거", "그것", "위에서 말한" 같은 참조 이해
- VS Code Copilot Chat처럼 대화 흐름 유지
- 각 채팅방이 독립적인 메모리 보유
- 맥락 이해를 통한 향상된 사용자 경험

---

## RAG 구성

### 텍스트 청킹
```python
RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", " ", ""]
)
```

### 임베딩
- 모델: `text-embedding-3-small`
- 제공자: OpenAI
- 벡터 저장소: ChromaDB (로컬 영구 저장)

### 생성
- 모델: `gpt-4o-mini`
- Temperature: `0.0` (결정적)
- Max Tokens: `1000`

### 대화 히스토리 ✅ 신규
- 최대 히스토리 쌍: `10` (`MAX_CONVERSATION_HISTORY`로 설정 가능)
- 메시지 잘림: 500자 이상 메시지는 잘림 처리
- 정렬: 시간순 (오래된 것부터 최신 순)
- 범위: 채팅 세션별 (독립적 컨텍스트)

---

## 오류 처리

### 문서 처리 오류
- 문서 상태가 `PROCESSING`인 경우: RAG에 사용하지 않음 (필터링됨)
- 문서 상태가 `FAILED`인 경우: RAG에 사용하지 않음 (필터링됨)
- 문서가 존재하지 않는 경우: 404 오류 반환

### RAG 오류
- ChromaDB 실패 시: 일반 대화로 폴백
- OpenAI API 실패 시: 사용자에게 오류 메시지 반환
- 임베딩 실패 시: RAG를 건너뛰고 일반 대화 사용

### 데이터베이스 오류
- 실패 시 트랜잭션 롤백
- 클라이언트에게 적절한 오류 메시지 반환
- 채팅/메시지 생성 실패는 원자적으로 처리

---

## 성능 고려사항

### 백그라운드 처리
- 문서 처리는 비동기적으로 실행됨 (FastAPI BackgroundTasks)
- 사용자는 `status=processing`으로 즉시 응답 받음
- 처리는 백그라운드에서 진행:
  1. 텍스트 추출 (PDF/MD/TXT)
  2. 청킹 (RecursiveCharacterTextSplitter)
  3. 임베딩 생성 (OpenAI API)
  4. 벡터 저장 (ChromaDB)
  5. 상태를 `completed` 또는 `failed`로 업데이트

### 캐싱
- RAG 서비스는 싱글톤 패턴 사용 (embeddings, vectorstore, llm)
- ChromaDB는 디스크에 영구 저장 (서버 재시작 시에도 유지)
- 재시작 시 문서 재처리 불필요

### 데이터베이스 최적화
- 소프트 삭제 (`deletedAt` 필드)
- 인덱스된 필드: `chatId`, `userId`, `documentId`
- 모든 목록 엔드포인트에 페이지네이션

---

## 향후 개선 사항

### 잠재적 개선사항
1. **시맨틱 캐싱**: 유사한 쿼리를 캐시하여 API 호출 감소
2. **하이브리드 검색**: 벡터 검색과 키워드 검색 결합
3. **다중 문서 합성**: 여러 문서에 걸친 정보를 더 잘 처리
4. **대화 요약**: 토큰 절약을 위한 긴 대화 자동 요약
5. **스트리밍 응답**: 더 나은 UX를 위해 토큰 단위로 AI 응답 스트리밍
6. **고급 인용**: 정확한 페이지 번호 표시 및 관련 텍스트 하이라이트
7. **컨텍스트 우선순위**: 히스토리에서 중요한 메시지 식별 및 우선순위 지정
8. **사용자 제어 히스토리**: 사용자가 채팅별로 MAX_CONVERSATION_HISTORY 설정 가능
